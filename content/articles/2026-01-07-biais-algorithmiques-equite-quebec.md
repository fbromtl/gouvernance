---
title: "Biais algorithmiques et équité : les enjeux pour le Québec"
slug: biais-algorithmiques-equite-quebec
date: "2026-01-07"
category: analyse
excerpt: "Les algorithmes ne sont ni neutres ni objectifs. Ils reflètent et amplifient les biais sociaux. Pour le Québec, société attachée à l'équité et aux droits de la personne, la lutte contre les biais algorithmiques est un enjeu fondamental."
cover: /images/articles/biais-algorithmiques.jpg
author: florian-brobst
featured: false
tags: "biais, équité, discrimination, algorithmes, Charte des droits, inclusion, justice sociale"
---

## Le miroir déformant des algorithmes

Les systèmes d'intelligence artificielle, contrairement à une idée répandue, ne sont pas neutres ni objectifs. Ils reflètent — et parfois amplifient — les biais présents dans les données sur lesquelles ils sont entraînés, dans les choix de conception de leurs développeurs et dans les structures sociales qu'ils modélisent. La question des biais algorithmiques est un enjeu de gouvernance fondamental pour le Québec, une société qui affirme des valeurs d'équité, d'inclusion et de respect des droits de la personne.

## Comprendre les biais algorithmiques

Les biais algorithmiques peuvent se manifester à chaque étape du cycle de vie d'un système d'IA. Au stade de la collecte des données, un jeu de données qui sous-représente certains groupes de la population produira un modèle moins performant pour ces groupes. Au stade de la conception du modèle, les choix de variables, de métriques d'optimisation et d'architecture peuvent incorporer des présupposés discriminatoires. Au stade du déploiement, l'interaction entre le système et son environnement peut engendrer des dynamiques de renforcement des inégalités.

Il est important de distinguer différents types de biais. Les biais de représentation résultent d'un déséquilibre dans les données d'entraînement. Les biais de mesure proviennent d'instruments ou de méthodes de collecte de données qui ne mesurent pas ce qu'ils sont censés mesurer de manière équitable pour tous les groupes. Les biais historiques reflètent des inégalités passées ou présentes inscrites dans les données. Les biais d'agrégation surviennent lorsqu'un modèle unique est appliqué à des populations hétérogènes sans tenir compte de leurs différences.

## Les manifestations dans le contexte québécois

Au Québec, les biais algorithmiques peuvent se manifester dans de nombreux secteurs et affecter différents groupes de la population.

Les communautés racisées et les personnes issues de l'immigration sont particulièrement exposées aux biais algorithmiques dans les domaines de l'emploi, du logement, du crédit et des services publics. Des systèmes d'IA utilisés pour le tri de candidatures, l'évaluation du risque de crédit ou l'allocation de logements sociaux peuvent reproduire des schémas discriminatoires enracinés dans les données historiques.

Les Premiers Peuples du Québec — les Premières Nations et les Inuit — sont confrontés à des risques spécifiques. La sous-représentation des populations autochtones dans les jeux de données, combinée aux effets cumulatifs de la discrimination systémique, peut conduire à des systèmes d'IA qui desservent ces communautés de manière disproportionnée. L'utilisation de l'IA dans les services sociaux, la justice et la santé peut avoir des conséquences graves pour des populations déjà marginalisées.

Les personnes en situation de handicap peuvent être discriminées par des systèmes d'IA qui ne prennent pas en compte la diversité des capacités et des modes d'interaction. Les systèmes de reconnaissance vocale moins performants pour les personnes ayant des troubles de la parole, les interfaces visuelles inaccessibles aux personnes malvoyantes ou les algorithmes de recrutement pénalisant les parcours professionnels atypiques sont autant d'exemples de biais d'exclusion.

Les femmes et les personnes de la diversité de genre peuvent être affectées par des biais algorithmiques dans les systèmes de recrutement, de recommandation et de décision automatisée. Des études ont montré que certains systèmes d'IA associent des stéréotypes de genre aux professions, aux compétences et aux comportements.

Les francophones peuvent être désavantagés par des systèmes d'IA dont les performances sont optimisées pour l'anglais. Ce biais linguistique, bien que souvent non intentionnel, constitue une forme de discrimination systémique dans le contexte québécois.

## Le cadre juridique applicable aux biais algorithmiques

La Charte des droits et libertés de la personne du Québec interdit la discrimination fondée sur la race, la couleur, le sexe, l'identité ou l'expression de genre, la grossesse, l'orientation sexuelle, l'état civil, l'âge, la religion, les convictions politiques, la langue, l'origine ethnique ou nationale, la condition sociale, le handicap ou l'utilisation d'un moyen pour pallier ce handicap. Cette interdiction s'applique aux décisions prises avec l'aide de systèmes d'IA autant qu'aux décisions humaines.

La Commission des droits de la personne et des droits de la jeunesse dispose du mandat d'enquêter sur les pratiques discriminatoires et de promouvoir les droits protégés par la Charte. Son rôle pourrait s'avérer déterminant dans l'examen des pratiques algorithmiques discriminatoires. Toutefois, la capacité de la Commission à évaluer les systèmes d'IA complexes nécessite le développement de compétences techniques spécialisées.

La Loi 25 contribue indirectement à la lutte contre les biais algorithmiques en imposant la transparence des décisions automatisées et le droit de les contester. La possibilité pour une personne de connaître les facteurs ayant mené à une décision algorithmique la concernant est un outil précieux pour détecter d'éventuels biais.

## Les approches techniques de détection et d'atténuation

La communauté de recherche québécoise, notamment au sein de Mila et d'IVADO, contribue activement au développement de méthodes techniques pour détecter et atténuer les biais algorithmiques.

Les audits algorithmiques permettent d'évaluer systématiquement les performances d'un système d'IA pour différents groupes de la population. En mesurant les écarts de performance — taux d'erreur, taux de faux positifs, taux de faux négatifs — entre les groupes, ces audits peuvent révéler des biais qui ne seraient pas apparents dans une évaluation globale du système.

Les techniques de débiaisage visent à réduire les biais à différentes étapes du processus d'apprentissage. Le prétraitement des données peut corriger des déséquilibres de représentation. L'apprentissage sous contraintes d'équité impose des critères de non-discrimination dans l'optimisation du modèle. Le post-traitement des résultats ajuste les seuils de décision pour équilibrer les performances entre les groupes.

Ces techniques ne sont toutefois pas des solutions magiques. Le choix d'une définition d'équité — et il en existe de nombreuses, parfois mutuellement incompatibles — est un choix de valeurs, pas un choix technique. Les organisations doivent engager une réflexion éthique sur la définition d'équité la plus appropriée à leur contexte, en consultation avec les parties prenantes affectées.

## Vers une gouvernance proactive de l'équité algorithmique

Les organisations québécoises doivent adopter une approche proactive pour prévenir et corriger les biais algorithmiques. Cela implique d'intégrer l'analyse d'équité dès la conception des systèmes d'IA, de constituer des équipes de développement diversifiées, reflétant la diversité de la population québécoise, de réaliser des audits d'équité réguliers tout au long du cycle de vie des systèmes, de mettre en place des mécanismes de signalement accessibles pour les personnes estimant être victimes de discrimination algorithmique, de documenter et de publier les résultats des évaluations d'équité, et de consulter les communautés potentiellement affectées dans la conception et l'évaluation des systèmes.

Le gouvernement du Québec pourrait jouer un rôle moteur en imposant des évaluations d'impact sur l'équité pour les systèmes d'IA déployés dans le secteur public, en soutenant la recherche sur les biais algorithmiques dans le contexte québécois, en finançant le développement d'outils et de méthodologies d'audit adaptés et en établissant des normes d'équité algorithmique pour les marchés publics.

## Conclusion

Les biais algorithmiques ne sont pas une fatalité, mais les combattre exige une vigilance constante, des compétences techniques et éthiques, et une volonté politique affirmée. Le Québec, par ses valeurs d'équité et d'inclusion et par l'expertise de son écosystème de recherche en IA, a les moyens de devenir un leader dans la promotion de l'équité algorithmique. Mais cela suppose de passer de la reconnaissance du problème à l'action concrète, systématique et mesurable.
